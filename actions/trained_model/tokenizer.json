{
  "version": "0.0.1",
  "truncation": null,
  "padding": null,
  "added_tokens": [],
  "special_tokens": [
    {
      "id": 0,
      "special_token": true,
      "content": "<pad>",
      "single_word": false,
      "lstrip": false,
      "rstrip": false,
      "normalized": true,
      "name": "pad_token"
    },
    {
      "id": 1,
      "special_token": true,
      "content": "<unk>",
      "single_word": false,
      "lstrip": false,
      "rstrip": false,
      "normalized": true,
      "name": "unk_token"
    },
    {
      "id": 2,
      "special_token": true,
      "content": "<s>",
      "single_word": false,
      "lstrip": false,
      "rstrip": false,
      "normalized": true,
      "name": "cls_token"
    },
    {
      "id": 3,
      "special_token": true,
      "content": "</s>",
      "single_word": false,
      "lstrip": false,
      "rstrip": false,
      "normalized": true,
      "name": "sep_token"
    },
    {
      "id": 4,
      "special_token": true,
      "content": "<mask>",
      "single_word": false,
      "lstrip": false,
      "rstrip": false,
      "normalized": true,
      "name": "mask_token"
    }
  ],
  "tokenizer_class": "T5Tokenizer",
  "model_max_length": 512,
  "init_inputs": [],
  "normalizer": null,
  "pre_tokenizer": null,
  "post_processor": null,
  "decoder": null,
  "added_tokens_encoder": {},
  "added_tokens_decoder": {},
  "vocab": {},
  "merges": []
}
